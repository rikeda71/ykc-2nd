{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nfrom scipy.sparse import csr_matrix\nimport gensim\nimport os\nos.listdir(\"../input/ykc-cup-2nd/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## read data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/ykc-cup-2nd/train.csv\")\ntest = pd.read_csv(\"../input/ykc-cup-2nd/test.csv\")\nsub = pd.read_csv(\"../input/ykc-cup-2nd/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## trainとtestをくっつけて一括で特徴量作成をする\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"department_id\"] == 3].head()\n## 野菜とか果物？","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"department_id\"] == 12].head()\n## 調味料？","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"department_id\"] == 16].head()\n##洗濯用具とか","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stopword & normalization\n# stopword settings\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n# normalizing setting\n# https://yukinoi.hatenablog.com/entry/2018/05/29/120000\nimport re\nshortened = {\n    '\\'m': ' am',\n    '\\'re': ' are',\n    'don\\'t': 'do not',\n    'doesn\\'t': 'does not',\n    'didn\\'t': 'did not',\n    'won\\'t': 'will not',\n    'wanna': 'want to',\n    'gonna': 'going to',\n    'gotta': 'got to',\n    'hafta': 'have to',\n    'needa': 'need to',\n    'outta': 'out of',\n    'kinda': 'kind of',\n    'sorta': 'sort of',\n    'lotta': 'lot of',\n    'lemme': 'let me',\n    'gimme': 'give me',\n    'getcha': 'get you',\n    'gotcha': 'got you',\n    'letcha': 'let you',\n    'betcha': 'bet you',\n    'shoulda': 'should have',\n    'coulda': 'could have',\n    'woulda': 'would have',\n    'musta': 'must have',\n    'mighta': 'might have',\n    'dunno': 'do not know',\n    # 実データからの置換\n    'softgels': \"soft gels supplement\",\n    \"almondmilk\": \"almond milk\",\n    \"lunchables\": \"lunch\",\n    \"febreze\": \"deodorant\",\n    'steamfresh': 'steam fresh',\n    \"lil\": 'little',\n    'volumizing': 'volume',\n    'rigate': 'penne',\n    'anticavity': 'cavity protection',\n    'keurig': 'coffee',  # わからん\n    'eggo': 'waffle',  # 要確認\n    'pantiliners': 'panty liner',\n    'nutri': 'nutrition',  # 要確認\n    'havarti': 'cheese',\n    'lunchables': 'lunch',\n    '0.0oz': 'deodorant',\n    '0oz': 'deodorant',\n    'velveeta': 'cheese',\n    'organix': 'organic',  # シャンプーかドッグフード？\n    'muenster': 'cheese',\n    'smartblend': 'smart blend dog pet',  # dog foods?\n    'snickerdoodle': 'cookie',\n    '0ct': 'diamond ring',  # カラット\n    'grands!': 'cat pet',  # cat foods?\n    'umcka': 'supplement',\n    'marzano': 'pizza tomato',\n    'butterfinger': 'candy chocolate peanut',  # バターフィンガー\n    'modena': 'italy',\n    'unstopables': 'deodorant',\n    'yokids': 'sandal',\n    \"flamin'\": 'gay',  # ?\n    'beneful': 'dog pet',\n    'swaddlers': 'diapers',\n    'compleats': 'meal preserved',  # 保存食?\n    'sambucus': 'drink berry',\n    'lindor': 'chocolate gift',\n    'macrobar': 'chocolate peanut',\n    'honeycrisp': 'apple',  #?\n    'ahoy!': 'europe',  #ヨーロッパの挨拶?\n    'whips!': \"whip\", # 鞭?\n    'arrabbiata': 'tomato sauce',\n    'craisins': 'dry berry fruit',\n    'nyquil': 'medicine',\n    'actionpacs': 'detergent',\n    'sproutofu': 'organic teriyaki tofu', # ??\n    'chewables': 'medicine',\n    'gurt': 'pig',\n    'krunch': 'chocolate',\n    'doubleshot': 'coffee',\n    'activia': 'yogurt',\n    'fillo': 'pillow',\n    'snax': 'snack',  # わからん\n    'snackimals': 'snack organic',\n    'oxiclean': 'bleach detergent',\n    'chex': 'cheese', # ??\n    'tahitian': 'tahiti',\n    'montebello': 'oil', # ??\n    'vegenaise': 'seasoning spice',\n    'noticeables': 'deodorant',\n    'scoopable': 'cat pet', # ??\n    'wetjet': 'clean',\n    'pantene': 'shampoo',\n    'shirataki': 'noodle',  # 白滝はnoodleでいいの?\n    'triscuit': 'snack',\n    'dophilus': 'capsule supplement',\n    'danimals': 'sweet', # 甘い食品を売ってそうなブランド\n    'purina': 'cat dog pet',  # ペットフード\n    'creamline': 'hamburger',\n    'funfetti': 'cookie',  # おいしくなさそうなクッキー?\n    'friskies': 'cat pet',\n    'krinkle': 'biscuit',\n    'antigingivitis': 'mouse wash tooth',\n    'nesquik': 'chocolate drink milk',\n    'sleepytime': 'herb tea',\n    'gillette': 'shaving shaver shave',\n    'antiplaque': 'tooth mouse wash',\n    'detangler': 'treatment shampoo',\n    'wintermint': 'gum',\n    'perspirant': 'deodorant',\n    'clorox': 'deodorant',\n    'multimineral': 'multi mineral',\n    'hommus': 'bean',\n    'steamables': 'potato vegetable',\n    'dentastix': 'dog pet',\n    'nutrish': 'dog pet',\n    '0st': '0',\n    '0nd': '0',\n    '0rd': '0',\n}\n\nshortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')\n\ndef get_shortended_word(word: str):\n    \"\"\"\n    単語の正規化\n    \"\"\"\n    \n    \n    shortened_word = re.sub(r\"[0-9]+\", \"0\", word)  # 数字\n    shortened_word = shortened_re.sub(lambda w: shortened[w.group(0)], shortened_word) # 置換\n    shortened_word = re.sub(r\"'(s|n)\", \"\", shortened_word)  # 's, 'n を削除\n    shortened_word = re.sub(r\"(%|\\\\|!|\\(|\\)|#|\\.|™|\\\"|\\'|®)\", \"\", shortened_word)  # いろいろ削除\n    shortened_word = re.sub(r\"(\\+|\\-|\\/|:)\", \" \", shortened_word)  # +, -など  を空白に置換\n    return shortened_word.split(\" \")\n\ndef flatten(l):\n    \"\"\"\n    2,3d list => 1d list\n    [[1,2], 3, [4,5]] => [1,2,3,4,5]\n    \"\"\"\n    \n    for el in l:\n        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el\n\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words : [get_shortended_word(w) for w in words])\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words: list(set([w for word in words for w in word if w not in stop_words])))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# その他の特徴量\ndf[\"name_num\"] = df[\"product_name\"].apply(len)  # product name の数\ndf[\"in_num\"] = df[\"product_name\"].apply(lambda words: any([c.isdigit() for w in words for c in w]))  # product nameに数字を含んでいるか\n# train_data での order_dow_mode の出現割合\ndow_counter = collections.Counter(train['order_dow_mode'].tolist())\ndow_rate = {k: v / len(train) for k, v in dow_counter.items()}\ndf[\"dow_rate\"] = df[\"order_dow_mode\"].apply(lambda x: dow_rate[x])\n# train_data での order_hour_of_day_mode の出現割合\nday_counter = collections.Counter(train['order_hour_of_day_mode'].tolist())\nday_rate = {k: v / len(train) for k, v in day_counter.items()}\ndf[\"day_rate\"] = df[\"order_hour_of_day_mode\"].apply(lambda x: day_rate[x])\n# order_rate の値の大きさ\ndef get_order_rate_basis(order_rate: float):\n    if order_rate > 5e-4:\n        return 0\n    elif order_rate > 1e-4:\n        return 1\n    elif order_rate > 5e-5:\n        return 2\n    elif order_rate > 1e-5:\n        return 3\n    elif order_rate > 5e-6:\n        return 4\n    elif order_rate > 1e-6:\n        return 5\n    elif order_rate > 5e-7:\n        return 6\n    elif order_rate > 1e-7:\n        return 7\n    else:\n        return 8\ndf[\"order_rate_basis\"] = df[\"order_rate\"].apply(get_order_rate_basis)\n\nimportant_words = {\n    0: ['cream', 'ice', 'chicken', 'pizza', 'frozen', 'cheeze', 'chocolate', 'vanilla', 'gluten'],\n    1: ['sleep', 'liquid', 'melatonin', 'baby', 'mix', 'tablets', 'flavor', 'natural', 'hand'],\n    2: ['bread', 'whole', 'grain', 'tortillas', 'buns', 'gluten', 'rolls', 'chocolate'],\n    3: ['baby', 'red', 'salad', 'bag', 'potato', 'potatoes', 'lettuce', 'sweet', 'green', 'apple', 'mashrooms'],\n    4: ['wine', 'beer', 'ale', 'sauvignon', 'callfornia', 'cabernet', 'lager', 'chardonnay', 'red', 'whiskey', 'ponot'],\n    5: ['sauce', 'rice', 'noodles', 'noodle', 'thai', 'soup', 'miso', 'curry', 'spacy', 'sesame', 'medium'],\n    6: ['tea', 'juice', 'water', 'coffee', 'drink', 'green', 'sparking','soda', 'orange', 'lemmon', 'ginger'],\n    7: ['cat', 'dog', 'chicken', 'beef', 'treat', 'treats', 'turkey', 'adult', 'flavor', 'dry', 'tuna', 'salmon'],\n    8: ['pasta', 'rice', 'sauce', 'cheese', 'whole', 'grain', 'spaghetti', 'macaroni', 'chicken', 'garlic', 'brown', 'tomato'],\n    9: ['rice', 'bean', 'beans', 'granola', 'brown', 'super', 'cranberry', 'mung', 'rolled', 'oats', 'pesto', 'sauce', 'berry'],\n    10: ['body', 'shampoo', 'oil', 'conditioner', 'wash', 'deodorant', 'soap', 'vitamin', 'hand', 'with','tablets', 'mint'],\n    11: ['chicken', 'sausage', 'smoked', 'beef', 'turkey', 'bacon', 'boneless', 'pork', 'breast', 'franks', 'ground', 'uncured'],\n    12: ['dressing', 'mix', 'sauce', 'butter', 'oil', 'seasoning', 'suger', 'honey', 'salsa', 'chocolate', 'ground', 'salt'],\n    13: ['cereal', 'granola', 'oatmeal', 'gluten', 'honey', 'mix', 'pancake', 'cinnamon', 'chocolate', 'grain', 'instant'],\n    14: ['soup', 'beans', 'bean', 'tomatoes', 'tomato', 'vegetable', 'tuna', 'water', 'whole', 'white', 'sauce'],\n    15: ['cheese', 'yogurt', 'milk', 'fat', 'greek', 'vanilla', 'cheddar', 'lowfat', 'strawberry', 'cream', 'berry', 'original'],\n    16: ['scent', 'cleaner', 'detergent', 'laundry', 'liquid', 'fresh', 'paper', 'bags', 'ultra', 'dish', 'fabric', 'lavender'],\n    17: ['baby', 'food', 'stage', 'diapers', 'apple', 'banana', 'size', 'foods', 'food', 'yogurt', 'fruit', 'wipes'],\n    18: ['chocolate', 'bar', 'chips', 'chip', 'dark', 'cookies', 'cookie', 'cracker', 'crackers', 'salt', 'butter', 'sea'],\n    19: ['humms', 'hum', 'turkey', 'chicken', 'salad', 'roasted', 'roast', 'breast', 'deli', 'tofu', 'salami', 'dip'],\n    20: ['yogurt', 'chocolate', 'cheese', 'strawberry', 'apple', 'chicken', 'fruit', 'vanilla', 'original', 'cream', 'potato']\n}\n\nimportance_feature_names = []\nfor k, v in important_words.items():\n    importance_feature_names.append(\"important_\" + str(k) + \"_rate\")\n    # importance_feature_names.append(\"important_\" + str(k) + \"_flag\")\n    df[\"important_\" + str(k) + \"_rate\"] = df['product_name'].apply(lambda words: sum([1 for w in words if w in v]) / len(words))\n    # df[\"important_\" + str(k) + \"_flag\"] = df['product_name'].apply(lambda words: any([True if w in v else False for w in words]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 訓練済みの単語ベクトルを読み込んで，product_nameに含まれる単語をベクトルに変換して平均を取ることで，各product_idに対して特徴量ベクトルを作成する\n\n## gensimで.vecから読み込むときに時間がかかるので，他のnotebookでpickleで保存したものを使用している\nmodel = pd.read_pickle(\"../input/ykc-cup-2nd-save-fasttext/fasttext_gensim_model.pkl\") \n\n## gensimでvecから読み込む場合（５分ぐらいかかる）\n# model = gensim.models.KeyedVectors.load_word2vec_format('../input/ykc-2nd/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec')\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\nlemmatizer = nltk.WordNetLemmatizer() # レンマ化\ndef to_vec(x, model):\n    cnt = 0\n    v = np.zeros(model.vector_size)\n    all_pretrained_words = model.index2word\n\n    for w in x:\n        cnt += 1\n        # lemmatizeは遅いので try except で必要な単語だけlemmatizeする\n        try:\n            v += model[w] ## 単語が訓練済みモデルのvocabにあったら\n        except:\n            try:\n                # 存在しない場合は、レンマ化したものが訓練済みモデルのvocabにあるかを確認\n                lemmatized_w = lemmatizer.lemmatize(w)\n                v += model[lemmatizer.lemmatize(w)]\n            except:\n                cnt -= 1\n                unused_words[w] += 1 ## ベクトルが存在しなかった単語をメモ\n                \n    v /= cnt if cnt > 0 else 1  # SWEM average-pooling\n    v = v / (np.sqrt(np.sum(v ** 2)) + 1e-16) ## 長さを1に正規化\n    return v\n\nvecs = df[\"product_name\"].apply(lambda x : to_vec(x, model))\nvecs = np.vstack(vecs)\nfasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\nvec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(unused_words.items(), key=lambda x: x[1], reverse = True)[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# config\nimport torch\nimport torch.nn as nn\n!pip install skorch\nimport skorch\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import Callback, Checkpoint, EarlyStopping\ntorch.manual_seed(42)\n\nclass MLPModel(nn.Module):\n    def __init__(self, num_features, dropout=0.25, n_hid=128):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(num_features, n_hid),\n            nn.ReLU(),\n            nn.BatchNorm1d(n_hid),\n            nn.Dropout(dropout),            \n            nn.Linear(n_hid, n_hid // 4),\n            nn.ReLU(),\n            nn.BatchNorm1d(n_hid // 4),\n            nn.Dropout(dropout),\n            nn.Linear(n_hid // 4, 21),  # 21 class\n        )\n        self.softmax = nn.Softmax(dim=-1)\n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, input_tensor):\n        return self.softmax(self.model(input_tensor))\n\n\nimport torch.nn.functional as F\n\n\n# https://github.com/kefirski/pytorch_Highway/blob/master/highway/highway.py\nclass Highway(nn.Module):\n\n    def __init__(self, size: int, num_layers: int, f: torch.nn.functional):\n        \"\"\"\n        :param size: linear layer size\n        :param num_layers: number of linear layers\n        :param f: activation function (ex. F.softmax, F.ReLU)\n        \"\"\"\n\n        super(Highway, self).__init__()\n        self.num_layers = num_layers\n        self.nonlinear = nn.ModuleList(\n            [nn.Linear(size, size) for _ in range(num_layers)])\n        self.linear = nn.ModuleList(\n            [nn.Linear(size, size) for _ in range(num_layers)])\n        self.gate = nn.ModuleList([nn.Linear(size, size)\n                                   for _ in range(num_layers)])\n        self.f = f\n\n    def forward(self, x):\n        \"\"\"\n        :param x: tensor with shape of (batch_size, size)\n        :return: tensor with shape of (batch_size, size)\n        \"\"\"\n\n        for layer in range(self.num_layers):\n            gate = torch.sigmoid(self.gate[layer](x))\n            nonlinear = self.f(self.nonlinear[layer](x))\n            linear = self.linear[layer](x)\n            x = gate * nonlinear + (1 - gate) * linear\n\n        return x\n\n\nclass CNNModel(nn.Module):\n\n    def __init__(self, num_features: int,\n                 cnn_filter_sizes: list=[1,3,5,10], cnn_num_filters: list=[100,200,300,400],\n                 highway_layers_num: int = 1, dropout: float = 0.5):\n        \"\"\"\n        :param embedding: word embedding\n        :param emb_dim: number of word embedding dimension\n        :param cnn_filter_sizes: filter sizes of CNNs\n        :param cnn_num_filters: filter numbers of CNNs\n        :param highway_layers_num: numbers of highway network layer\n        :param dropout_rate: drop out rate\n        \"\"\"\n\n        super().__init__()\n        self.cnn_filter_sizes = cnn_filter_sizes\n        self.cnn_num_filters = cnn_num_filters\n\n        self.emb_dim = num_features\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, self.emb_dim))  # nn.Conv1d(n, f, emb_dim)\n            for (n, f) in zip(cnn_num_filters, cnn_filter_sizes)\n        ])\n        self.highway = Highway(sum(cnn_num_filters),\n                               highway_layers_num, nn.ReLU())\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(sum(cnn_num_filters), 21)  # 21 = class num\n        self.softmax = nn.LogSoftmax(dim=0)\n        self._init_parameters()\n\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        Forward propagation algorithm.\n        :param x: embeddings\n        :return: (batch_size, 2)\n        \"\"\"\n\n        convs = [F.relu(conv(x).squeeze(3))\n                 for conv in self.convs]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2)\n                 for conv in convs]\n        pred = torch.cat(pools, 1)\n        pred = self.highway(pred)\n        return self.softmax(self.linear(self.dropout(pred)))\n\n    def _init_parameters(self):\n        for param in self.parameters():\n            param.data.uniform_(-0.05, 0.05)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 予測に使用する特徴量の名前\nfeatures = fasttext_pretrain_cols + [\n    \"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\",  # 元から用意されている素性\n    'name_num', 'in_num',  # product name に関する素性\n    'dow_rate', 'day_rate','order_rate_basis'  # その他の特徴量を変形した素性\n] + importance_feature_names  # 各カテゴリごとの重要単語が出現するかどうかとその割合みたいなもの\ntarget = \"department_id\" ## 予測対象\nn_split = 7 ## cross validationのfold数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## trainとtestを分離\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cross validation\nfrom sklearn.ensemble import VotingClassifier\npreds_test = []\nscores = []\nkfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\npreds = []\nanss = []\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    ## train LGBM model\n    lgbm_params = {\n        'n_estimators': 700,\n        'objective': 'multiclass',\n        \"boosting_type\": \"gbdt\",\n        \"importance_type\": \"split\",\n        \"random_state\": 42,\n        'num_leaves': 225,\n        'learning_rate': 0.04689606818793407,\n        'class_weight': None,\n        'min_child_samples': 98,\n        'subsample': 0.44354721466773056,\n        'subsample_freq': 6,\n        'colsample_bytree': 0.7377347290625655,\n        'reg_alpha': 1.0160018949956453,\n        'reg_lambda': 1.6781461339752908,\n        'n_jobs': 2\n    }\n    lgbm_model = LGBMClassifier(**lgbm_params)\n    lgbm_model.fit(x_tr, y_tr, eval_set=(x_va, y_va), early_stopping_rounds=10)\n    ## predict on valid\n    pred_val = lgbm_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")\n    }\n    print(score)\n    scores.append(score)\n    preds.append(np.argmax(pred_val, axis = 1))\n    anss.append(y_va)\n        \n    ## predict on test\n    pred_test = lgbm_model.predict_proba(test[features].to_numpy().astype(np.float32))\n    preds_test.append(pred_test)\n    # \"\"\"\n\n    monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\n \n    # set param(make trainer)\n    neural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=200,\n                    lr=0.005,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    callbacks=[Checkpoint(), EarlyStopping(patience=10)],\n                    module__num_features=train[features].shape[1],\n                    # module__n_hid=512,\n                    module__dropout=0.25,\n                    iterator_valid__batch_size=256,\n                    device=\"cuda\"\n                )\n    neural_model.fit(x_tr.to_numpy().astype(np.float32), y_tr.to_numpy().astype(np.int64))\n    ## predict on valid\n    pred_val = neural_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \n    # voting\n    \"\"\"\n    estimators = [\n        ('lgbm', lgbm_model),\n        ('mlp', neural_model)\n    ]\n    voting_model = VotingClassifier(estimators, n_jobs=-1)\n    voting_model.fit(x_tr.to_numpy().astype(np.float32), y_tr.to_numpy().astype(np.int64))\n    pred_val = neural_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \"\"\"\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")\n    }\n    print(score)\n    scores.append(score)\n    preds.append(np.argmax(pred_val, axis = 1))\n    anss.append(y_va)\n        \n    ## predict on test\n    pred_test = neural_model.predict_proba(test[features].to_numpy().astype(np.float32))\n    preds_test.append(pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluate for each class\nprint(classification_report(np.concatenate(anss), np.concatenate(preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df = pd.DataFrame(scores)\nscore_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"## cvの各foldで計算した予測値の平均を最終的な予測値に\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame(\n    model.feature_importances_,\n    index=features,\n    columns=['importance']\n)\n\nimportance = importance.sort_values('importance', ascending=False)\nimportance.head(50).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# configs\nimport optuna\nimport json\nimport datetime as dt\n\nn_trials = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def param_grids_to_params(trial: optuna.Trial, param_grids: dict):\n    params = {}\n    for k, v in param_grids.items():\n        # set optimizing target parameters\n        if isinstance(v, list):\n            if len(v) > 2:\n                params[k] = trial.suggest_categorical(k, v)\n            elif all([isinstance(s, bool) for s in v]):\n                b = strtobool(trial.suggest_categorical(k, [str(p) for p in v]))\n                params[k] = True if b == 1 else False\n            elif type(v[0]) == int:\n                params[k] = trial.suggest_int(k, v[0], v[1])\n            elif type(v[0]) == float:\n                params[k] = trial.suggest_uniform(k, v[0], v[1])\n            else:\n                params[k] = trial.suggest_categorical(k, v)\n        # set static parameters\n        else:\n            params[k] = v\n    return params\n\n\ndef objective(trial: optuna.Trial):\n    fmeasures = []\n    model = LGBMClassifier\n    param_grids = {\n            \"boosting_type\": \"gbdt\",\n            \"num_leaves\": [2, 256],\n            \"max_depth\": -1,\n            \"learning_rate\": [0.005, 0.1],\n            \"n_estimators\": 500,\n            \"subsample_for_bin\": 200000,\n            \"objective\": \"multiclass\",\n            \"class_weight\": [\"balanced\", None],\n            \"min_split_gain\": 0.0,\n            \"min_child_weight\": 0.001,\n            \"min_child_samples\": [5, 100],\n            \"subsample\": [0.4, 1.0],\n            \"subsample_freq\": [1, 7],\n            \"colsample_bytree\": [0.65, 1.0],\n            \"reg_alpha\": [1e-8, 10.0],\n            \"reg_lambda\": [1e-8, 10.0],\n            \"random_state\": 0,\n            \"n_jobs\": 2,\n            \"silent\": True,\n            \"importance_type\": \"split\",\n        }\n    params = param_grids_to_params(trial, param_grids)\n    kfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\n    for i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):    \n        ## train data\n        x_tr = train.loc[train_idx, features]\n        y_tr = train.loc[train_idx, target]\n\n        ## valid data\n        x_va = train.loc[valid_idx, features]\n        y_va = train.loc[valid_idx, target]\n\n        model = LGBMClassifier(**params)\n        model.fit(x_tr, y_tr, eval_set=(x_va, y_va), early_stopping_rounds=10)\n    \n        ## predict on valid\n        pred_val = model.predict_proba(x_va)\n        fmeasures.append(f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\"))\n        \n        break\n\n    f1score = sum(fmeasures) / len(fmeasures)\n    return f1score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=n_trials)\nparams = study.best_trial.params\nbest_score = study.best_value\nnow = dt.datetime.now()\njson_name = \"tuned_{0:%Y%m%d%H%M%S}_{1}.json\".format(now, \"lgbm\")\nwith open(json_name, \"w\") as f:\n    json.dump(params, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport json\n\n# monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\nneural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=100,\n                    lr=0.01,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    module__num_features=train[features].shape[1],\n                    device='cuda'\n                )\n# deactivate skorch-internal train-valid split and verbose logging\nneural_model.set_params(train_split=False, verbose=0)\nparams = {\n    'lr': [0.01],\n    'module__dropout': [0.25],\n    'module__n_hid': [128, 256, 512],\n}\ngs = GridSearchCV(neural_model, params, cv=7, scoring='f1_micro')\n\ngs.fit(train[features].to_numpy().astype(np.float32), train[target].to_numpy().astype(np.int64))\nprint(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\nwith open('neural_tuned_params.json', 'w') as f:\n    json.dump(gs.best_params_, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport json\n\n# monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\nneural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=10,\n                    lr=0.01,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    module__num_features=train[features].shape[1],\n                    device='cuda'\n                )\n# deactivate skorch-internal train-valid split and verbose logging\nneural_model.set_params(train_split=False, verbose=0)\nparams = {\n    'lr': [0.01],\n    'module__dropout': [0.25],\n    'module__n_hid': [128, 256, 512],\n}\ngs = GridSearchCV(neural_model, params, cv=3, scoring='f1_micro')\n\ngs.fit(train[features].to_numpy().astype(np.float32), train[target].to_numpy().astype(np.int64))\nprint(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\nwith open('neural_tuned_params.json', 'w') as f:\n    json.dump(gs.best_params_, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}